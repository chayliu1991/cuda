# 并行算法设计

## 并行基本概念

目前，主流的处理器首先都采用了**指令级并行**的技术，即“**乱序执行处理器**”：同一时刻存在着多条指令同时执行，执行指令的顺序无须和汇编代码给出的指令顺序一致。指令级并行较难使用，一般不作为优化首选方法，而**向量化**和**多核并行**则是常见的两种并行技术。向量化是在一个核心上使用同一条指令同时操作多个数据；多核并行是同时使用一个芯片上集成的多个核心对多个数据执行相同或不同的指令。

向量化的实现模式主要有SIMD（single instruction multiple data）和SIMT（single instruction multiple thread），X86架构和ARM架构的CPU，都是SIMD模式，而主流的GPU则采用SIMT模式。以Intel的CPU为例，向量化可以通过长向量处理器实现，一个256位的长向量处理器最多可以同时处理4个double型数据。

多核并行则主要与**线程**、**进程**的概念相关，一个处理器上可以通过**多线程**（multi-thread）并行的方式同时调用全部的核心，而**多进程**（multi-process）并行则可以在一个或多个处理器上申请多组线程共享资源和系统资源，每个进程里独立地执行一部分的计算任务，包括每个进程发起各自的多线程并行。

**缓存**（cache）是并行计算中另一个不能轻视的维度。由于一次内存访问需要的时间周期（200时钟周期以上）相比于常用浮点计算（个位数时钟周期甚至不到）非常长，现代处理器通过多种方式来减小内存访问延迟的影响。缓存就是其中一种能够提供更快数据访问速度的方式：通过不同级别大小不一的缓存，逐级改进内存访问速度。一次内存访问如果访问的数据在缓存中，则称为缓存命中，而**缓存命中率**是影响程序性能的一个重要因素。在并行计算中，对缓存的优化是必不可少的。

## 算法性能分析

**时间复杂度**和**空间复杂度**是常用的分析算法性能的度量标准，分别度量算法运行的性能和要使用的存储器空间大小。学习编程时首先就需要学会如何通过这两个标准分析算法。

时间复杂度是很好的评价算法优劣的标准，但不是一个很好的性能优化度量标准，主要原因有：

- 没有考虑读写对性能的影响
- 没有考虑处理器处理不同指令的速度差异
- 忽略常数和低阶运算对性能的影响
- 不能很好的度量并行算法。

空间复杂度也是一个比较相对的概念，只能衡量程序运行时需要的大致数据量，无法衡量程序运行时存储器访问次数，也没有考虑缓存层次结构的影响。

常用的度量性能的标准有**时间**、**FLOPS**、**延迟**、**吞吐量**等。

时间是最简单的判断优劣的标准，在同一台机器上，运行时间短的程序一般来说是更优的。计时的方法有很多，比如标准C库的time、clock系列函数，CUDA的event，Linux下的gettimeofday，Windows下的GetTickCount等。

FLOPS（floating-point operations per second）表示硬件每秒执行的浮点运算的数量，通常使用的单位为G（10^9）。FLOPS所测的实际上是“浮点运算器”（FPU）的速度，比如超级计算机基准程序Linpack就是采用FLOPS表示性能。但由于FLOPS不区分运算指令、不区分IO、缓存与计算，因此大多数程序的实际性能通常远小于硬件的FLOPS峰值性能。**效率**（Efficiency）就是一个程序在一个硬件上的实际性能除以这个硬件的峰值性能。

延迟是指一条指令、一个操作从开始发射到完全结束所经历的时间。通常说一个指令需要几个时钟周期完成，这个周期数就是延迟。现代CPU采用的延迟优化技术有：乱序执行、缓存和多线程等。

吞吐量是指在单位时间内，硬件能够完成的操作数。对于浮点操作，吞吐量就意味着FLOPS，对于访存来说，吞吐量表示在单位时间内硬件能够读写的字节数量。吞吐量倒数表示平均下来某种指令最少需要的时钟周期数，是一种下限，而延迟则是一个上限。在不考虑其他指令（如访存操作）影响的情况下，假设一个代码中有500条整数加法指令，由吞吐量倒数决定的下限是125个时钟周期，由延迟决定的上限是500个周期。

**加速比**（speedup ratio）是最简单的衡量代码优化获得的性能提升的方法。其表示优化前程序的运行时间与优化后运行时间的比值。

加速比可以衡量并行化的效果，而**并行效率**可以说明并行优化的可扩展性。并行效率定义为加速比与计算单元数的比例，假设一个程序在并行化后在8核处理器上获得的加速比为5，按么并行效率为0.625。一般而言，如果并行效率低于0.5就说明并行化是失败的（这可能意味着双核的性能还比不上单核，不过对于几十个甚至更多的核心又是另一回事）。并行效率越高说明可扩展性越好。

**Amdahl定律**描述了在固定问题规模的前提下，对某个模块获得了加速比*S*后对程序整体性能的提升。假设并行的部分在未优化前占整体比例为*f*，则程序的整体加速比*S*’为：

$$
S^{\prime}=\frac{1}{1-f+\frac{f}{S}}=\frac{1}{1-(1-1 / S) f}
$$

假设某个程序中，你优化了80%的代码，对这80%的代码你获得了加速比10，那么对整个程序而言，你的优化获得的加速比为：1/(1-0.8+0.8/10)=3.57，这远小于10。由Amdahl定律可知，如果只优化80%的代码，整个程序可获得的加速比上限为5。也就是说，如果一个10s的程序，无论如何优化其中1s的模块，程序总耗时也不会少于9s。因此Amdahl定律提供了由串行->并行优化的性能上限。

**Gustafson定律**描述了增加处理器数目同时增大问题规模对加速比的影响。假设并行的部分在未优化前占整体比例为*f*，使用了*n*个核心进行加速，则程序的整体加速比*S*’为：
$$
S^{\prime}=n-f(n-1)
$$
通过分析程序性能能够指导程序的热点在哪里，进而有针对性的优化程序，可以达到事半功倍的效果。对于某个确定的代码段，可以使用一些工具来分析硬件计数器，从而获得更详细的性能数据。

Linux下的性能分析工具有**perf**、**gprof**、**valgrind**等，**nvprof**可以用来分析NVIDIA公司GPU上CUDA程序的性能，**vampireTrace**能够在Linux和Windows上运行分析MPI、OpenMP甚至CUDA程序的性能，**Intel VTune**能够分析程序的性能瓶颈、函数的调用关系等。

## 串行代码优化

串行代码性能优化与并行代码优化同等重要，甚至更重要，一方面因为并行单独获得的加速通常有限，而串行代码优化有时能获得成千上万倍的加速；另一方面因为单个并行控制流的内部依旧是串行的，一般串行优化措施依然有效。

串行代码的优化由上至下可以分为：系统级别、应用级别、算法级别、函数级别、循环级别、语句级别、指令级别这7个层次，下面分别列举每个层次的优化手段。

### 系统级别

优化程序的性能，首先需要在系统级别找出程序的性能控制因素，再做针对性的优化。具体需要考虑的因素有：1、网络速度、利用率及负载均衡；2、处理器利用率；3、存储器带宽利用率；4、去阻塞处理器运算的因素。

### 应用级别

应用级别的优化通常应当最先采用，一方面因为它可能关系到程序的各个部分，另一方面也是因为随着性能优化工作的推进，越不容易采用应用级别的优化。应用级别的优化手段有：

- 编译器选项
- 调用高性能库
- 去掉全局变量
- 受限的指针
- 条件编译

### 算法级别

算法级别的优化通常涉及程序的一个部分，这个部分可以是一个或多个函数，甚至是一段语句，需要考虑数据的组织、算法实现的策略。数据组织主要需要缓存的优化，可以通过改变索引顺序和缓存分块优化缓存利用；软件预取能够隐藏访存延迟；而查表法是一种有效的算法实现优化。

### 函数级别

函数调用时，需要将调用参数通过寄存器或栈传递，且将函数返回地址入栈。函数级别的优化通常用于减少这部分的消耗及其导致的优化障碍。函数调用参数的优化一是通过指针传递大的结构体或类，二是尽可能避免使用全局变量。此外，使用内联函数（inline）能够消除函数调用的开销。

### 循环级别

循环如果执行次数多，很容易称为性能瓶颈。通常循环级别的优化以发掘循环的并行性、减少寄存器和缓存的使用为主，包括：

- 循环展开
- 循环积累
- 循环合并
- 循环拆分

### 语句级别

实现同一功能的不同语句编译后的指令数量不同、指令类型也不同，必然导致性能也不相同。对于语句级别的优化来说，需要尽量避免语句生成不需要的指令，或者让语句生成更加高效的指令，具体包括：

- 减少内存读写
- 选用尽量小的数据类型
- 结构体对齐
- 表达式移除
- 分支优化
- 优化交换性能

### 指令级别

不同的指令具有不同的吞吐量，在实现相同功能的前提下，使用高吞吐量的指令能够明显提升程序的性能。提高吞吐量的方法有：

- 减少数据依赖
- 注意处理器的多发射能力
- 优化乘除法和模余
- 选择更具体的库函数或算法

## 算法依赖分析

依赖是指程序代码必须按照某种顺序执行，如果不依据这种顺序就会产生不同的运行结果。依赖分析可以找出程序代码中哪些部分必须串行执行，哪些部分可以并行执行。根据粒度依赖可以分为**指令级依赖**和**循环级依赖**，指令级依赖分析是优化流水线性能的基础，而循环级依赖分析则是向量化和数据化并行的基础。如果程序不存在依赖，则可以达到接近理想的并行优化效果，一些技术手段则可以在存在依赖的情况下使程序获得更好的优化效果及正确的计算结果。

指令级依赖主要有以下几个方面：结构化依赖、数据依赖、控制依赖。

循环级依赖分为循环内部依赖，和循环间依赖。循环内部依赖与指令级依赖一致，循环间依赖指下一次循环依赖上一次循环的计算结果，包括：循环数据依赖、循环控制依赖。

通过去除代码中的依赖，可以增强代码的流水线执行、指令级并行或线程级并行能力。常见的去除依赖的技术有：

- 使用寄存器来保存有依赖的存储器访问，将存储器依赖转化为寄存器读写依赖，降低依赖对性能的影响
- 使用临时存储器来保存有依赖的读写，并要时再合并
- 重新组织代码顺序，尽量使得对每个元素的处理在某一次循环内完成

## 并行编程模型

并行编程模型大多数以数据和任务为中心来命名，一方面，并行编程模型是建立在硬件体系结构模型之上的并行程序实现逻辑的抽象，另一方面，是指并行算法设计时对模块间通信方式的抽象。这些并行模型适用的场景各不相同，存在彼此重叠的地方，一个并行应用可能属于多个不同的并行模型。下面分别列举不同尺度的并行编程模型。

### 指令级并行

如果多条指令之间既没有数据和控制依赖，也没有结构化依赖，那么它们可以同时在处理器的多个流水线上同时执行，这称为指令级并行。指令级并行的实现方法有：

- 修改源码以便让编译器生成需要的指令
- 汇编
- 使用编译器内置函数（intrinsic）

指令级并行优化粒度太细，需要深入了解处理器的流水线延迟和吞吐量以及编译器的能力，因此不易于使用。

### 向量化并行

向量化并行通常是对不同的数据执行一条同样的指令（即SIMD），或者说一个指令作用于一个数组/向量。主流的处理器都支持向量指令集，如X86处理器的SSE/AVX指令，ARM处理器的NEON指令。编译器通常提供编译制导（如OpenMP中的#pragma omp simd）和内置函数（对汇编指令进行简单的C封装）的方式对这些向量指令提供支持。NVIDIA的CUDA和开发的OpenCL标准则通过SIMT来提供一份代码既支持多核并行又支持向量化。

### 易并行**

易并行（embarrassingly parallel）是指并行指令的多个控制流之间没有通信的并行。比如图像的二值化操作中，对每个像素的操作都和其他像素无关，是完全独立的。这样，易并行算法的设计通常比较简单，可获得近似线性的性能提升。

### 任务并行

任务并行是指每个控制流计算并行任务中的一个子任务，通常其粒度比较大且较少通信甚至没有。任务并行和人类的思维方式比较类似，易于在原有的串行代码的基础上实现。使用任务并行时，负载不均衡的可能性非常大，可以采用任务队列的方式解决这个问题。

### 数据并行

数据并行是指一条指令同时作用在多个数据，那么可以将一个或多个数据分配给一个控制流计算，这样多个控制流就可以并行。数据并行时也需要考虑每个控制流负载不一定均衡的问题，可以通过动态调度基本达到负载均衡。

###  循环并行

很多算法迭代地处理大量数据，这表现为循环。如果某次循环必须等待前次循环执行完成才能执行，称之为串行循环，如果各次循环之间不存在依赖可以并行执行，称之为并行循环。通常而言，串行循环不能并行化，但某些串行循环可变成并行循环，某些看起来不能并行的串行算法已经有很好的并行算法（如stream compaction、reduction、scan等）。对于并行循环来说，向量化、线程级并行和多级其并行都可能适用。

###  流水线并行

流水线并行能够让不同的硬件单元同时运行以提高计算能力，常用队列来保存在某一阶段可并行执行的任务。比如在CUDA编程模型中，数据传输和GPU计算可以同时进行。

### 区域分解并行

将大的计算区域划分成多个小的区域，然后由一个控制流计算一个小的区域，那么计算大区域的所有控制流便可以同时进行，这称为区域分解并行。区域分解并行通常和具体的物理现象联系在一起（如流场网格的计算），但是一些其他的运算也和区域分解类似，如分块矩阵转置和分块矩阵乘法。

### 隐式和显示并行

如果编写显示并行代码的工作由开发者承担，这称为显示并行，如使用pthread开发并行代码。隐式并行通常是指开发人员通过编译器添加某种标记，指定程序的并行性，而实际的并行代码由编译器生成，如OpenMP和OpenACC。相对来说，显示并行易于控制，易于获得高效率，但移植性不好，工作量大，难以调试，而隐式并行正好相反。在现在的编程环境中运行两种方式混合使用，可以先使用隐式并行，必要的地方使用显示并行。

### 共享存储器并行

共享存储器并行是指所有控制流都能够访问一个共同的全局存储器，通过这个存储器来交换数据。目前基于多核并行处理器的并行环境基本上都是共享存储器并行环境。

### 分布式存储器并行

通过网络互联的多机系统、存储器分布在不同节点机上，每个节点机运行各自的操作系统，拥有独立的物理地址空间。由于通过网络互连，控制流之间的通信只能通过网络数据分发实现。分布式存储并行模型通过显示的消息传递来交换数据，天然适合这一类应用。目前常用的分布式存储并行环境是MPI。

## 并行编程环境

并行程序环境设计主要是从编程方式和通信方式上分类。以编程方式分为显示并行环境和隐式并行环境，以通信方式分类分为共享存储器并行环境和消息传递并行环境。下面介绍几种常见的并行编程环境：

### MPI

MPI（Message Passing Interface，消息传递接口）是一种显示消息传递编程环境。MPI定义了一组通信函数，以将数据从一个MPI进程发送到另一个MPI进程，即MPI实现了多进程并行。MPI是集群（Cluster）采用的主要编程方式，其可扩展性非常好。MPI只规定了标准并没有给出实现，目前主要的实现有MPICH、Mvapich、OpenMPI和InterMPI等。

### OpenMP

OpenMP（Open Multi-Processing）是一个基于隐式共享存储器的并行环境，能够实现多线程并行和向量化并行等。OpenMP支持C/C++/Fortran绑定，也被实现为库。目前常用的编译环境gcc、icc和Visual Studio都支持OpenMP。

###  fork/pthread

fork是类UNIX系统的一个调用，它调用一次，返回两个进程，一个是调用父进程，另一个是新产生的子进程，子进程会继承父进程的虚拟空间、打开的文件等。

pthread是一个基于线程的库，提供了创建、回收线程的函数。pthread创建的线程和父线程共享内存和指令，但有其独立的指令指针。

###  CUDA

CUDA是用于发挥NVIDIA GPU通用计算能力的显示共享存储器编程环境。

###  OpenCL

OpenCL（Open Computing Language，开发计算语言）是显示共享存储器的并行环境，先由Apple设计，后来交由Khronos Group维护，是异构平台平行编程的开发标准，也是一个编程框架。OpenCL不但支持数据并行，还支持任务并行。OpenCL的应用范围比CUDA广，能够运用在CPU、GPU和FPGA上，但OpenCL的API参数比较多，不支持函数重载，因此函数相对难以熟记。

### OpenACC

OpenACC是隐式共享存储器的并行环境，其编译器依据C/C++/Fortran编写的编译制导语句，将并行区域的代码翻译成另一种语言的表示，如CUDA、OpenCL等。软件开发人员只要在不同的操作系统和主机设备之间编译OpenACC代码，就可以实现可移植性。

###  NEON

NEON是隐式共享存储器的并行环境，它提供ARM处理器上的SIMD指令，一个指令可以同时对多个数据进行操作，同时操作的数据个数由向量寄存器的长度和数据类型共同决定。

###  SSE/AVX

SSE/AVX是隐式共享存储器的并行环境，它是Intel推出的用以挖掘SIMD能力的汇编指令。由于汇编编程太难，后来Intel又给出了其内置函数版本（intrinsic）。SSE/AVX指令支持数据并行，一个指令可以同时对多个数据进行操作，同时操作的数据个数由向量寄存器的长度和数据类型共同决定。

## 并行算法设计

设计并行算法时，思路与串行算法会有很大的区别，这是因为：串行性能最佳的算法可能并行化后效果不理想，某些看起来并行时间复杂度很好的算法实现后性能可能一般。一个好的并行算法通常具备这些特点：

- 具有并行性的部分恰好是热点
- 可扩展性好
- 易于实现。通常并行算法的设计涉及：划分、通信、归并和负载均衡

### 划分

划分的目的是将计算任务分成多个部分，以便多个控制流同时处理。通常划分分为两种：任务划分和数据划分。任务划分就是比如将吃饭和看电视两个任务分开，数据划分则是比如将稻田里的稻谷分由10台机器同时收割。划分的核心思想就是经典算法中的分治，即将大问题分解成小问题，通过求解小问题，再将小问题的结果组合起来以解决大问题。串行编程中，分治经常通过递归实现，这就还有优化空间：使用多个控制流并行地解决小问题，这符合并行算法的设计。

划分的方法直接影响并行算法的优劣，其原则有：

- 尽量使算法映射到硬件上后，各控制流处理的数据或任务不相关
- 减少通信消耗。常见的划分方法有：均匀划分、递归划分、指数下降划分等

### 通信

现实中易并行的计算非常少，大多数并行算法都需要在控制流之间进行通信，这通常是因为要对某些计算步骤的结果进行合并处理。相比串行算法来说，通信是并行算法引入的额外消耗，如果能够减少这种消耗就能够提高并行效率，获得更好的可扩展性。下面是一些减少通信消耗的办法：

- 算法设计时尽可能减少通讯代价
- 减少通信次数
- 使用异步通信，边计算边通信
- 每次通信传输尽量多的数据，以减少通信的准备时间
- 将通信分散到多个组件上避免某个组件称为瓶颈
- 尽量使用轻量级的通信方式
- 减少由于通信导致的执行流等待

### 归并

对于并行计算来说，每个控制流计算后得到的结果可能不是最终需要的，不同控制流计算的结果之间可能存在重叠、依赖等等，为了获得最终需要的结果，就要对各个控制流计算得到的结果进行归并。和通信一样，结果归并相比于串行代码，也是额外的部分。结果归并的代码占用时间越少，可扩展性就越好。

### 负载均衡

负载均衡是指通过调整计算在各个处理器上的分配，以充分发挥系统内处理器的计算能力，通常这意味着各个处理器近似同时结束计算。好的并行算法应当具有好的负载均衡，因为负载不均衡会导致效率的降低。负载均衡算法主要分成两种：

静态负载均衡，是指在程序运行前，软件开发人员已经将计算资源尽可能均匀地分配给各个控制流运行，通常在作用在各个数据上的操作或处理任务的时间近似相等时使用。

动态负载均衡，是指在程序运行过程中，显示地重新调整任务的分布以达到负载均衡的目的。

## 并行算法缺陷

并行和串行之间本质的区别只是由单个控制流执行代码转向了多个控制流同时执行代码，但这种简单的转变导致了许多并行不同于串行的问题和可能的性能缺陷。这些问题和缺陷中的大部分是为了提升并行算法的性能，或不让不能并行的代码能够并行而引入的，但是使用不当的话，却可能使程序性能降低。常见的引入并行的缺陷有：

### 启动结束时间

由于缓存的刷新，操作系统启动、调度，和结束进程或线程都是耗时的操作，故通常不建议在CPU上频繁创建和结束进程、线程，而是一次创建，多次重复使用。

### 负载均衡

由于很难保证分配给不同的控制流的任务计算量完全相等，故其计算完成时间可能并不完全相同，这使得先完成任务的控制流必须等待。通常解决负载均衡问题的方法是降低任务的粒度，然后在各控制流之间动态分配任务。

###  竞写

多个控制流操作共享变量时，非常容易出现竞写问题，发生竞写时的结果和多个控制流的运气有关。通常解决竞写问题有两个策略：

- 不共享数据
- 对共享数据的读写使用锁或原子操作等

###  锁

锁通常用来解决竞写问题，使得原本不能并行的算法能够并行。由于锁会导致竞争锁的多个控制流串行运行，且多个控制流竞争锁会引入更多内存访问，故导致性能下降。因此，通常要求锁住的临界区要尽量小。

死锁是指多个控制流相互拥有对方请求的资源的同时请求对方持有的资源，如果没有外力改变，这种状态将永远持续下去。死锁是一种算法设计缺陷，目前只能依靠软件开发人员自身的智慧，要求软件开发人员在设计算法时候排除死锁的可能。

###  饿死

饿死是指某个控制流一直得不到计算，本质上是一种负载均衡问题。大多数情况下可能和优先级有关，另外就是软件开发人员的失误。

###  伪共享

伪共享是指核心上所有的访问都通过内存完成，这是因为为了保持缓存一致性，如果某个核心更新了缓存线的数据，那么其他缓存该数据的核心就必须使得它们缓存的该数据的缓存失效。伪共享使程序的性能从缓存的性能降到内存或共享缓存的性能。在使用原子操作解决竞写时，由于多个控制流同时更新一个数据，原子操作会导致严重的伪共享，这是原子操作慢的原因之一。

###  原子操作

原子操作通常保证多个控制流同时操作的结果和每个控制流依次串行的操作的结果一致，其概念在某种意义上与CPU流水线上的一次执行一条指令的抽象相冲突。由于频繁需要刷新流水线，原子操作的延迟通常在几百个时钟周期，而且通常随着处理器数目的增加，代价也成倍增大。

###  存储器栅栏

为了缩小存储器访问和处理器之间速度的差异，现代计算机使用了大量多层次的缓存。假设某个核心需要读取另一个核心刚写入存储器的共享数据，为了保证多个控制流看到的存储器地址空间上的数据时完全一致的，需要保证：1、所有控制流都执行到相同的代码；2、所有控制流对存储器地址空间的更新都已经完成且对其他控制流可见，存储器栅栏提供了这种保证。如果某个控制流需要访问不久前一个控制流更新后的数据，就需要调用存储器栅栏。存储器栅栏引入了两种类型的消耗：1、控制流之间相互等待；2、减弱了缓存的作用。

###  缓存一致性

在处理器还只有单核心的时代，缓存一致性便已存在，其目的是保证内存中和缓存中的数据是一致的。多核CPU同样实现了这种机制，保证了多核CPU中已被缓存的一个地址的读操作一定会返回那个地址最新的值。

###  顺序一致性

顺序一致性一般是指多个控制流运行同一段代码可以得到唯一的结果，即无论各个控制流是以何种顺序运行代码，结果应带是一致的。实际上，这不是一个必须严格遵守的条件，有时为了多个控制流运行，允许结果有一定的误差。

###  volatile同步错误

如果两个线程需要同时访问一个变量，为了让其中两个线程每次都能读到这个变量的最新值，就把它定义为volatile。虽然volatile意味着每次读操作和写操作都是直接操作内存，但是volatile在现有C/C++标准中不保证原子性，许多用volatile来进行多线程同步的方案都是错的。

## 并行设计准则

设计并行算法时，可以根据如下14条准则进行。

- 查找算法的热点并优先考虑热点的并行化
- 找出算法中的并行性
- 自外而内或自内而外
- 注意算法的通信计算比
- 选择并行算法时不要只看评价串行算法的标准
- 采用合适的并行模型使算法更好地映射到硬件上
- 尽可能利用已有的并行库
- 注意并行程序的可扩展性
- 永远不要假设具体的执行顺序
- 注意通信开销
- 使用线程私有变量
- 注意锁的粒度
- 全局设计
- 步步验证












